{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m5WMW7NT3wT6"
   },
   "outputs": [],
   "source": [
    "!pip install requests langchain-community beautifulsoup4 langchain chromadb sentence-transformers together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Io--cum37rR"
   },
   "source": [
    "# **Scraping**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fvC5bSdN36AA"
   },
   "outputs": [],
   "source": [
    "# scraping import\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "# ingestion import\n",
    "import chromadb\n",
    "from chromadb import Client\n",
    "from chromadb.config import Settings\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import gensim.downloader\n",
    "from google.colab import drive\n",
    "import json\n",
    "import requests\n",
    "import base64\n",
    "import gzip\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hGFkQgjr3-sh"
   },
   "outputs": [],
   "source": [
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "POggwbUP4AJx"
   },
   "outputs": [],
   "source": [
    "url = 'http://hrlibrary.umn.edu/instree/ainstls1.htm'\n",
    "response = requests.get(url)\n",
    "if response.status_code == 200:\n",
    "    print(\"Successfully fetched the webpage.\")\n",
    "else:\n",
    "    print(f\"Failed to fetch webpage. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vyy7MTw84BVR"
   },
   "outputs": [],
   "source": [
    "def extract_links(html_content, base_url):\n",
    "    extracted_links =[]\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Extract all <a> tags\n",
    "    anchor_tags = soup.find_all('a')\n",
    "\n",
    "    # Loop through each <a> tag and extract href attribute\n",
    "    for tag in anchor_tags:\n",
    "        href = tag.get('href')  # Extract the href attribute\n",
    "        if href and (href.endswith('.htm') or href.endswith('.html')):\n",
    "            full_url = urljoin(base_url, href) # Join the base URL with relative links to get absolute URL\n",
    "            extracted_links.append(full_url)\n",
    "    return extracted_links\n",
    "\n",
    "extracted_links = list(extract_links(response.content,url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TDuUE1-l4DA5"
   },
   "source": [
    "# **Ingestion**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zqbCKRqA4UpB"
   },
   "source": [
    "extract the content from each link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NN-IZ9aU4GM_"
   },
   "outputs": [],
   "source": [
    "def extract_text_content(link):\n",
    "    response = requests.get(link)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Get all text from the page, stripping out script and style elements\n",
    "        for script in soup([\"script\", \"style\"]):\n",
    "            script.decompose()  # remove all scripts and styles\n",
    "\n",
    "        # Get text and strip leading/trailing whitespace\n",
    "        text_content = soup.get_text(separator='\\n', strip=True)\n",
    "        return text_content\n",
    "    else:\n",
    "        print(f\"Failed to retrieve document at {link}. Status code: {response.status_code}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S5ajServ4YiR"
   },
   "source": [
    "Embedding models initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BEof0hul4H2R"
   },
   "outputs": [],
   "source": [
    "# 1. Hugging Face Embedding\n",
    "hf_embedding_function = HuggingFaceEmbeddings(model_name=\"BAAI/bge-base-en-v1.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sVKAZiOu4NWB"
   },
   "outputs": [],
   "source": [
    "# 2. Word2Vec Embedding\n",
    "word2vec_model =  gensim.downloader.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Z9RgnvH4Oqh"
   },
   "outputs": [],
   "source": [
    "# 3. TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N_HvjV2A4mOR"
   },
   "source": [
    "functions to ingest data using the 3 embedding models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r6G4HipR4S95"
   },
   "outputs": [],
   "source": [
    "def ingest_huggingface(links):\n",
    "\n",
    "    collection = client.get_or_create_collection(name=\"rag_with_HF\")\n",
    "    for link in links:\n",
    "            doc_content = extract_text_content(link)\n",
    "            # Check if doc_content is None before proceeding\n",
    "            if doc_content is not None:\n",
    "                embedding = hf_embedding_function.embed_documents([doc_content])\n",
    "                doc_id = link  # Use the link as the document ID\n",
    "                collection.add(\n",
    "                    documents=[doc_content],\n",
    "                    embeddings=[embedding[0]],\n",
    "                    metadatas=[{\"source\": link}],\n",
    "                    ids=[doc_id]\n",
    "                )\n",
    "                print(f\"Ingested document with Hugging Face embedding: {link}\")\n",
    "            else:\n",
    "                print(f\"Skipping document with no content: {link}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zc0ul7--4vy5"
   },
   "outputs": [],
   "source": [
    "def ingest_word2vec(links):\n",
    "\n",
    "    collection = client.get_or_create_collection(name=\"rag_with_w2v\")\n",
    "    for link in links:\n",
    "        doc_content = extract_text_content(link)\n",
    "        if doc_content is not None:\n",
    "            tokens = doc_content.split()  # Tokenize content\n",
    "\n",
    "            embeddings = [word2vec_model[token] for token in tokens if token in word2vec_model.key_to_index]\n",
    "\n",
    "            doc_id = link  # Use the link as the document ID\n",
    "\n",
    "            if embeddings:\n",
    "                # Calculate the average of the embeddings\n",
    "                average_embedding = np.mean(embeddings, axis=0)\n",
    "\n",
    "                collection.add(\n",
    "                    ids=[doc_id],\n",
    "                    documents=[doc_content],\n",
    "                    embeddings=[average_embedding],\n",
    "                    metadatas=[{\"source\": link}]\n",
    "                )\n",
    "                print(f\"Ingested document with Word2Vec embedding: {link}\")\n",
    "            else:\n",
    "                print(f\"Skipping document with no valid tokens: {link}\")\n",
    "        else:\n",
    "            print(f\"Failed to retrieve or extract content from document at {link}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sN27rlb-4yOh"
   },
   "outputs": [],
   "source": [
    "def ingest_tfidf(links):\n",
    "    collection = client.get_or_create_collection(name=\"rag_with_TF\")\n",
    "    all_documents = []\n",
    "    doc_ids = []\n",
    "\n",
    "    for link in links:\n",
    "        doc_content = extract_text_content(link)\n",
    "        if doc_content is not None:\n",
    "            all_documents.append(doc_content)\n",
    "            doc_ids.append(link)\n",
    "        else:\n",
    "            print(f\"Failed to retrieve or extract content from document at {link}.\")\n",
    "\n",
    "    # Check if we have any valid documents before fitting the model\n",
    "    if all_documents:\n",
    "        # Fit the TF-IDF model on all documents\n",
    "        tfidf_vectorizer.fit(all_documents)\n",
    "        tfidf_embeddings = tfidf_vectorizer.transform(all_documents).toarray()\n",
    "\n",
    "        for idx, link in enumerate(doc_ids):\n",
    "            collection.add(\n",
    "                documents=[all_documents[idx]],\n",
    "                embeddings=[tfidf_embeddings[idx]],\n",
    "                metadatas=[{\"source\": link}],\n",
    "                ids=[doc_ids[idx]]\n",
    "            )\n",
    "            print(f\"Ingested document with TF-IDF embedding: {link}\")\n",
    "    else:\n",
    "        print(\"No valid documents to ingest.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4RA43AH75F2R"
   },
   "source": [
    "ingestion process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zrrkskPo6Oya"
   },
   "outputs": [],
   "source": [
    "# Initialize ChromaDB client\n",
    "client = Client(settings=Settings(persist_directory=\"/content/drive/My Drive/Chromadb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MXBSvXOg4-rx"
   },
   "outputs": [],
   "source": [
    "ingest_huggingface(extracted_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N8aayovA5AOB"
   },
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "for i in range(0, len(extracted_links), batch_size):\n",
    "    batch_links = extracted_links[i:i + batch_size]\n",
    "    ingest_word2vec(batch_links)\n",
    "    print(f\"Processed batch {i // batch_size + 1} of {len(extracted_links) // batch_size + 1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_0EJFxso5EAa"
   },
   "outputs": [],
   "source": [
    "ingest_tfidf(extracted_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YYxO4XLa4zX5"
   },
   "source": [
    "function to extract the data from each collection and upload it to github as JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nFmCFavl47Hf"
   },
   "outputs": [],
   "source": [
    "def extract_data_from_collection(collection_name):\n",
    "    collection = client.get_or_create_collection(name=collection_name)\n",
    "\n",
    "    # Retrieve all document IDs\n",
    "    documents = collection.get()\n",
    "    all_data = []\n",
    "\n",
    "    # Loop through and print each document's details\n",
    "    for doc_id in documents['ids']:  # Access the 'ids' key from the returned dictionary\n",
    "        # Get the document details using the document ID\n",
    "        document_details = collection.get(ids=[doc_id], include=['documents', 'embeddings', 'metadatas'])\n",
    "\n",
    "        content = document_details['documents'][0]\n",
    "        embedding = document_details['embeddings'][0]\n",
    "        source = document_details['metadatas'][0]['source']\n",
    "\n",
    "        # Combine them into a structured format\n",
    "        all_data.append({\n",
    "            \"document\": content,\n",
    "            \"embedding\": embedding,\n",
    "            \"metadata\": {\"source\": source},\n",
    "            \"id\": doc_id\n",
    "        })\n",
    "\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hZ14NrXR5KOh"
   },
   "outputs": [],
   "source": [
    "def upload_to_github(filename):\n",
    "    # Your GitHub details\n",
    "    # GITHUB_TOKEN = \"\"\n",
    "    # REPO_NAME = \"FarahSaleh121/chroma_collections\"\n",
    "\n",
    "    # Upload the file to GitHub\n",
    "    with open(filename, \"rb\") as f:  # Open file in binary mode\n",
    "        content = f.read()\n",
    "\n",
    "    # Encode content to Base64\n",
    "    encoded_content = base64.b64encode(content).decode('utf-8')\n",
    "\n",
    "    url_g = f\"https://api.github.com/repos/{REPO_NAME}/contents/{filename}\"\n",
    "\n",
    "    # Prepare the data for GitHub API\n",
    "    data = {\n",
    "        \"message\": f\"Add collection data batch {filename}\",\n",
    "        \"content\": encoded_content,\n",
    "    }\n",
    "\n",
    "    # Use the GitHub API to upload the file\n",
    "    response = requests.put(url_g, headers={\"Authorization\": f\"token {GITHUB_TOKEN}\"}, json=data)\n",
    "\n",
    "    if response.status_code == 201:\n",
    "        print(f\"File {filename} uploaded successfully!\")\n",
    "    else:\n",
    "        print(\"Error:\", response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hgoAkG9N5Mpq"
   },
   "outputs": [],
   "source": [
    "def upload_data_in_batches(filename,all_data, batch_size=400):\n",
    "    total_batches = len(all_data) // batch_size + (1 if len(all_data) % batch_size != 0 else 0)\n",
    "\n",
    "    for i in range(total_batches):\n",
    "        batch_data = all_data[i * batch_size : (i + 1) * batch_size]\n",
    "\n",
    "        # Prepare data for saving\n",
    "        collection_data = {\n",
    "            \"documents\": [item[\"document\"] for item in batch_data],\n",
    "            # Convert embeddings to lists before saving\n",
    "            \"embeddings\": [item[\"embedding\"].tolist() if isinstance(item[\"embedding\"], np.ndarray) else item[\"embedding\"] for item in batch_data],\n",
    "            \"metadatas\": [item[\"metadata\"] for item in batch_data],\n",
    "            \"ids\": [item[\"id\"] for item in batch_data],\n",
    "        }\n",
    "\n",
    "        # Create a filename for the current batch\n",
    "        current_filename = f\"{filename}_{i + 1}.json\"\n",
    "\n",
    "        # Save the current batch to a JSON file\n",
    "        with open(current_filename, \"w\") as f:\n",
    "            json.dump(collection_data, f)\n",
    "\n",
    "        # Upload the file to GitHub\n",
    "        upload_to_github(current_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EmYB8vmr5uZx"
   },
   "source": [
    "this function speciallized for TF-IDF because the data is larger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wjvum_275yyE"
   },
   "outputs": [],
   "source": [
    "def upload_data_in_compressed_batches(filename, all_data, batch_size=400):\n",
    "    total_batches = len(all_data) // batch_size + (1 if len(all_data) % batch_size != 0 else 0)\n",
    "\n",
    "    for i in range(total_batches):\n",
    "        batch_data = all_data[i * batch_size : (i + 1) * batch_size]\n",
    "\n",
    "        # Prepare data for saving\n",
    "        collection_data = {\n",
    "            \"documents\": [item[\"document\"] for item in batch_data],\n",
    "            \"embeddings\": [item[\"embedding\"].tolist() if isinstance(item[\"embedding\"], np.ndarray) else item[\"embedding\"] for item in batch_data],\n",
    "            \"metadatas\": [item[\"metadata\"] for item in batch_data],\n",
    "            \"ids\": [item[\"id\"] for item in batch_data],\n",
    "        }\n",
    "\n",
    "        # Create a filename for the compressed batch\n",
    "        compressed_filename = f\"{filename}_batch_{i + 1}.json.gz\"\n",
    "\n",
    "        # Compress and save the batch to a file\n",
    "        with gzip.open(compressed_filename, \"wt\", encoding=\"utf-8\") as f:\n",
    "            json.dump(collection_data, f)\n",
    "\n",
    "        # Upload the compressed file to GitHub\n",
    "        upload_to_github(compressed_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U7XxgjrX5XHS"
   },
   "outputs": [],
   "source": [
    "all_data_HF = extract_data_from_collection(\"rag_with_HF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7HWcnsHi5aSh"
   },
   "outputs": [],
   "source": [
    "upload_data_in_batches('rag_with_HF',all_data_HF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ynjIau7k5dVq"
   },
   "outputs": [],
   "source": [
    "all_data_w2v = extract_data_from_collection(\"rag_with_w2v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y76Yy3eZ5oUB"
   },
   "outputs": [],
   "source": [
    "upload_data_in_batches('rag_with_w2v',all_data_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rNUBCIZe5paJ"
   },
   "outputs": [],
   "source": [
    "all_data_TF = extract_data_from_collection(\"rag_with_TF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AMMNeJ9i53lL"
   },
   "outputs": [],
   "source": [
    "upload_data_in_compressed_batches(\"rag_with_TF\", all_data_TF)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNCm7BimqMhr/T6Z0OdAr/D",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
